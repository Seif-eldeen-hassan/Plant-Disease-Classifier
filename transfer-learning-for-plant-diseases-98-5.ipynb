{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":13437604,"datasetId":8529170,"databundleVersionId":14152926},{"sourceType":"datasetVersion","sourceId":150545,"datasetId":70909,"databundleVersionId":160811}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"text-align:center; font-family: 'Arial', sans-serif; margin-top: 30px; background-color:#f5f7fa; padding:25px; border-radius:15px; box-shadow: 0 4px 10px rgba(0,0,0,0.1);\">\n    <h1 style=\"color:#2E8B57; font-size: 50px; margin-bottom: 10px;\">Plant Disease Classifier</h1>\n    <p style=\"color:#555; font-size: 20px; margin-bottom:5px;\">by <strong>Seif Eldeen</strong></p>\n    <p style=\"color:#333; font-size: 18px; max-width: 850px; margin:auto; line-height:1.5;\">\n        This project demonstrates a deep learning model based on <strong>Transfer Learning</strong> using architectures such as   <strong>MobileNetV3</strong> to classify plant leaf images into <strong>15 different disease categories</strong>.\n        The dataset includes healthy and infected leaves from multiple plant species. The model was trained with advanced augmentation, early stopping, and fine-tuning to achieve high accuracy while preventing overfitting.\n    </p>\n    <hr style=\"width:50%; margin:auto; border:2px solid #2E8B57; margin-top:20px;\">\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#2E8B57; color:white; padding:10px 20px; border-radius:8px; font-family: 'Arial', sans-serif; margin-top:20px; margin-bottom:10px; box-shadow: 0 3px 6px rgba(0,0,0,0.1);\">\n    <h2 style=\"margin:0; font-size:24px;\">1) Importing Libraries</h2>\n    <p style=\"color:white; font-size:16px; margin-top:5px; line-height:1.4;\">\n        In this section, we import all the essential Python libraries for image preprocessing, visualization, and building our deep learning model using transfer learning for plant disease classification.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"!pip install split-folders","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T16:20:59.415572Z","iopub.execute_input":"2025-10-19T16:20:59.415729Z","iopub.status.idle":"2025-10-19T16:21:04.095154Z","shell.execute_reply.started":"2025-10-19T16:20:59.415714Z","shell.execute_reply":"2025-10-19T16:21:04.094228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom PIL import Image\nimport random\nimport splitfolders\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV3Large\nfrom tensorflow.keras.applications.mobilenet_v3 import preprocess_input\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras.models import load_model\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport tensorflow as tf\nfrom keras.utils import load_img, img_to_array\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:03:33.745261Z","iopub.execute_input":"2025-10-19T23:03:33.745858Z","iopub.status.idle":"2025-10-19T23:03:33.752151Z","shell.execute_reply.started":"2025-10-19T23:03:33.745835Z","shell.execute_reply":"2025-10-19T23:03:33.751370Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#2E8B57; color:white; padding:10px 20px; border-radius:8px; font-family: 'Arial', sans-serif; margin-top:20px; margin-bottom:10px; box-shadow: 0 3px 6px rgba(0,0,0,0.1);\">\n    <h2 style=\"margin:0; font-size:24px;\">2) Defining Constants</h2>\n    <p style=\"color:white; font-size:16px; margin-top:5px; line-height:1.4;\">\n        Here, we define important constants such as dataset paths, image size, batch size, and other parameters used throughout the training process to ensure consistency and easy configuration.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"IMAGES_FOLDERS_PATH = '/kaggle/input/plantdisease/PlantVillage'\nOUTPUT_FOLDER = 'plantdisease_split'\nBATCH_SIZE = 64\nWIDTH = 224\nHEIGHT = 224\nCHANNELS = 3\nEPOCHS = 100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T16:21:18.860807Z","iopub.execute_input":"2025-10-19T16:21:18.861405Z","iopub.status.idle":"2025-10-19T16:21:18.865200Z","shell.execute_reply.started":"2025-10-19T16:21:18.861385Z","shell.execute_reply":"2025-10-19T16:21:18.864445Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#2E8B57; color:white; padding:10px 20px; border-radius:8px; font-family: 'Arial', sans-serif; margin-top:20px; margin-bottom:10px; box-shadow: 0 3px 6px rgba(0,0,0,0.1);\">\n    <h2 style=\"margin:0; font-size:24px;\">3) Dataset Analysis</h2>\n    <p style=\"color:white; font-size:16px; margin-top:5px; line-height:1.4;\">\n        In this section, we perform an initial analysis of the dataset by visualizing sample images from each plant disease category.\n        The function <strong>VisualizeClasses()</strong> dynamically loads and displays one example from each folder, showing the class name and image resolution to verify the dataset structure and consistency.\n        Additionally, a random image is displayed alongside its resized version (224×224) to ensure correct preprocessing before training.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"def VisualizeClasses(folders_path):\n    # Get all class folders and sort them\n    ClassesNames = os.listdir(folders_path)\n    ClassesNames.sort(key=lambda x: (x.split('__')[0], x))\n    \n    # Display the total number of classes\n    NUM_CLASSES = len(ClassesNames)\n    print(f\"Classes Number: {NUM_CLASSES}\")\n    \n    # Prepare the grid for showing sample images\n    cols = 4\n    rows = (NUM_CLASSES + cols - 1) // cols\n    fig, axes = plt.subplots(rows, cols, figsize=(16, rows * 3))\n    axes = axes.flatten()\n    \n    # Show one sample image from each class\n    for i, cat in enumerate(ClassesNames):\n        ClassFolderPath = os.path.join(folders_path,cat)\n        ImagesName = os.listdir(ClassFolderPath)\n        img_path = os.path.join(ClassFolderPath,ImagesName[0])\n        with Image.open(img_path) as img:\n            width, height = img.size\n        image = mpimg.imread(img_path)\n        axes[i].imshow(image)\n        axes[i].set_title(f\"{cat}\\n{width}x{height}\", fontsize=10)\n        axes[i].axis('off')\n        \n    # Hide unused subplots if any\n    for j in range(i + 1, len(axes)):\n        axes[j].axis('off')\n\n# Call the function to visualize all classes\nVisualizeClasses(IMAGES_FOLDERS_PATH)   \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T16:21:18.865910Z","iopub.execute_input":"2025-10-19T16:21:18.866103Z","iopub.status.idle":"2025-10-19T16:21:22.145322Z","shell.execute_reply.started":"2025-10-19T16:21:18.866088Z","shell.execute_reply":"2025-10-19T16:21:22.144271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pick a random class folder\nClassesNames = os.listdir(IMAGES_FOLDERS_PATH)\nrandom_class = random.choice(ClassesNames)\nclass_path = os.path.join(IMAGES_FOLDERS_PATH, random_class)\n\n# Pick a random image from that class\nimages = os.listdir(class_path)\nrandom_image = random.choice(images)\nimg_path = os.path.join(class_path, random_image)\n\n# Open and resize the selected image to 224x224\nwith Image.open(img_path) as img:\n    resized_img = img.resize((WIDTH, HEIGHT)) \n\n# Display the original and resized images side by side\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\naxes[0].imshow(mpimg.imread(img_path))\naxes[0].set_title(\"Original\")\naxes[0].axis('off')\n\naxes[1].imshow(resized_img)\naxes[1].set_title(f\"Resized ({WIDTH}x{HEIGHT})\")\naxes[1].axis('off')\n\n# Adjust layout and show the figure\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T16:21:22.146428Z","iopub.execute_input":"2025-10-19T16:21:22.146786Z","iopub.status.idle":"2025-10-19T16:21:22.436127Z","shell.execute_reply.started":"2025-10-19T16:21:22.146755Z","shell.execute_reply":"2025-10-19T16:21:22.435415Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#2E8B57; color:white; padding:10px 20px; border-radius:8px; font-family: 'Arial', sans-serif; margin-top:20px; margin-bottom:10px; box-shadow: 0 3px 6px rgba(0,0,0,0.1);\">\n    <h2 style=\"margin:0; font-size:24px;\">4) Data Preparation</h2>\n    <p style=\"color:white; font-size:16px; margin-top:5px; line-height:1.4;\">\n        In this section, the dataset is split into <strong>training (80%)</strong> and <strong>validation (20%)</strong> sets using <strong>SplitFolders</strong>.\n        To improve the model's generalization and prevent overfitting, <strong>data augmentation</strong> techniques such as rotation, zooming, shearing, and horizontal flipping are applied using the <strong>ImageDataGenerator</strong> class.\n        All images are resized to <strong>224×224</strong> pixels and normalized through <strong>MobileNetV3-Large’s preprocessing function</strong>.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Split the dataset into training (80%) and validation (20%) folders\nsplitfolders.ratio(IMAGES_FOLDERS_PATH, output=OUTPUT_FOLDER, seed=42, ratio=(0.8, 0.2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T16:21:22.437007Z","iopub.execute_input":"2025-10-19T16:21:22.437297Z","iopub.status.idle":"2025-10-19T16:23:41.560607Z","shell.execute_reply.started":"2025-10-19T16:21:22.437273Z","shell.execute_reply":"2025-10-19T16:23:41.559860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the training and validation dataset paths\nTRAIN_FOLDER_PATH = '/kaggle/working/plantdisease_split/train'\nVAL_FOLDER_PATH = '/kaggle/working/plantdisease_split/val'\n\n# Data augmentation for training images (adds variations to improve generalization)\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input, # Apply MobileNetV3 preprocessing\n    rotation_range=20, # Random rotations\n    width_shift_range=0.2, # Horizontal shifts\n    height_shift_range=0.2, # Vertical shifts\n    shear_range=0.2, # Shear transformations\n    zoom_range=0.2,  # Random zoom\n    horizontal_flip=True,  # Flip images horizontally\n    fill_mode='nearest' # Fill missing pixels after transformations\n)\ntrain_image_generator = train_datagen.flow_from_directory(\n                                            TRAIN_FOLDER_PATH,\n                                            target_size=(WIDTH, HEIGHT), # Resize all images to 224x224\n                                            batch_size= BATCH_SIZE, # Number of images per batch\n                                            class_mode='categorical') # Multi-class classification\n\n\n# Validation images (no augmentation, only preprocessing)\nval_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\nval_image_generator = val_datagen.flow_from_directory(\n                                            VAL_FOLDER_PATH,\n                                            target_size=(WIDTH, HEIGHT),\n                                            batch_size= BATCH_SIZE,\n                                            class_mode='categorical')\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T16:23:41.561427Z","iopub.execute_input":"2025-10-19T16:23:41.561657Z","iopub.status.idle":"2025-10-19T16:23:41.859241Z","shell.execute_reply.started":"2025-10-19T16:23:41.561632Z","shell.execute_reply":"2025-10-19T16:23:41.858682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = list(val_image_generator.class_indices.keys()) \nclass_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T00:16:59.427370Z","iopub.execute_input":"2025-10-20T00:16:59.427955Z","iopub.status.idle":"2025-10-20T00:16:59.432973Z","shell.execute_reply.started":"2025-10-20T00:16:59.427929Z","shell.execute_reply":"2025-10-20T00:16:59.432371Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#2E8B57; color:white; padding:10px 20px; border-radius:8px; font-family: 'Arial', sans-serif; margin-top:20px; margin-bottom:10px; box-shadow: 0 3px 6px rgba(0,0,0,0.1);\">\n    <h2 style=\"margin:0; font-size:24px;\">5) Model Building</h2>\n    <p style=\"color:white; font-size:16px; margin-top:5px; line-height:1.4;\">\n        In this step, the <strong>MobileNetV3-Large</strong> architecture is used as the base model with pretrained <strong>ImageNet</strong> weights.\n        The base model’s layers are frozen to retain the learned features during initial training.\n        A <strong>Global Average Pooling</strong> layer is added to reduce spatial dimensions, followed by a <strong>Dense</strong> layer with 128 neurons and <strong>ReLU</strong> activation for deeper feature learning.\n        A <strong>Dropout</strong> layer with a rate of 0.3 helps prevent overfitting.\n        Finally, a <strong>Softmax</strong> output layer classifies images into the <strong>15 plant disease categories</strong>.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Load the MobileNetV3-Large model pre-trained on ImageNet (without the top classification layer)\nbase_model = MobileNetV3Large(weights='imagenet', include_top=False, input_shape=(WIDTH, HEIGHT, CHANNELS))\n\n# Freeze the base model layers to keep pretrained weights during initial training\nbase_model.trainable = False\n\n# Build the new model on top of the base model\nmodel = Sequential()\nmodel.add(base_model)\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(train_image_generator.num_classes,activation='softmax')) #Output layer for classification\n\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T16:23:41.859923Z","iopub.execute_input":"2025-10-19T16:23:41.860104Z","iopub.status.idle":"2025-10-19T16:23:45.281295Z","shell.execute_reply.started":"2025-10-19T16:23:41.860088Z","shell.execute_reply":"2025-10-19T16:23:45.280720Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#2E8B57; color:white; padding:10px 20px; border-radius:8px; font-family: 'Arial', sans-serif; margin-top:20px; margin-bottom:10px; box-shadow: 0 3px 6px rgba(0,0,0,0.1);\">\n    <h2 style=\"margin:0; font-size:24px;\">6) Model Training (Without Fine-Tuning)</h2>\n    <p style=\"color:white; font-size:16px; margin-top:5px; line-height:1.4;\">\n        In this phase, the model is trained using the <strong>MobileNetV3-Large</strong> base with frozen layers to retain pretrained ImageNet features.\n        The optimizer <strong>Adam</strong> is used with a learning rate of <strong>1e-4</strong>, and the loss function is <strong>categorical crossentropy</strong> for multi-class classification.\n        An <strong>EarlyStopping</strong> callback is applied to automatically stop training if no improvement in validation loss is observed for 5 consecutive epochs, restoring the best model weights.\n        After training, the model is saved as <strong>\"mobilenetv3_before_finetuning.keras\"\n","metadata":{}},{"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=5,              # Stop if no improvement for 5 epochs\n    restore_best_weights=True # Restore the model weights from the epoch with the best validation loss\n)\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhist = model.fit(\n    train_image_generator,\n    epochs = EPOCHS,\n    verbose = 1,\n    callbacks = early_stopping,\n    validation_data = val_image_generator\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T16:27:11.222888Z","iopub.execute_input":"2025-10-19T16:27:11.223577Z","iopub.status.idle":"2025-10-19T18:40:45.176898Z","shell.execute_reply.started":"2025-10-19T16:27:11.223556Z","shell.execute_reply":"2025-10-19T18:40:45.176237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained model before applying fine-tuning\nmodel.save(\"mobilenetv3_before_finetuning.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T18:43:01.748144Z","iopub.execute_input":"2025-10-19T18:43:01.748950Z","iopub.status.idle":"2025-10-19T18:43:02.237210Z","shell.execute_reply.started":"2025-10-19T18:43:01.748915Z","shell.execute_reply":"2025-10-19T18:43:02.236599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#2E8B57; color:white; padding:10px 20px; border-radius:8px; font-family: 'Arial', sans-serif; margin-top:20px; margin-bottom:10px; box-shadow: 0 3px 6px rgba(0,0,0,0.1);\">\n    <h2 style=\"margin:0; font-size:24px;\">7) Model Training (With Fine-Tuning)</h2>\n    <p style=\"color:white; font-size:16px; margin-top:5px; line-height:1.4;\">\n        After completing the initial training phase, fine-tuning is performed by <strong>unfreezing the last 20 layers</strong> of the <strong>MobileNetV3-Large</strong> base model.\n        This allows the network to slightly adjust its higher-level feature representations, improving adaptation to the plant disease dataset.\n        The optimizer <strong>Adam</strong> is recompiled with a lower learning rate of <strong>1e-5</strong> to ensure stable and controlled weight updates during fine-tuning.\n        The model is trained for <strong>10 additional epochs</strong> and saved as <strong>\"mobilenetv3_after_finetuning.keras\"</strong> for later comparison with the pre-fine-tuned version.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Unfreeze the last 30 layers of the base model to fine-tune higher-level features\nfor layer in base_model.layers[-30:]:\n    layer.trainable = True\n\n# Recompile the model with a smaller learning rate for fine-tuning\nmodel.compile(optimizer=Adam(learning_rate=1e-5),  \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train (fine-tune) the model again on the training data\nfine_tune_history = model.fit(\n    train_image_generator,\n    validation_data=val_image_generator,\n    epochs=EPOCHS,\n    callbacks = early_stopping,\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T18:46:24.017583Z","iopub.execute_input":"2025-10-19T18:46:24.018243Z","iopub.status.idle":"2025-10-19T21:24:44.649279Z","shell.execute_reply.started":"2025-10-19T18:46:24.018212Z","shell.execute_reply":"2025-10-19T21:24:44.648669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained model after applying fine-tuning\nmodel.save(\"mobilenetv3_after_finetuning.keras\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T21:27:38.470539Z","iopub.execute_input":"2025-10-19T21:27:38.470924Z","iopub.status.idle":"2025-10-19T21:27:39.027880Z","shell.execute_reply.started":"2025-10-19T21:27:38.470900Z","shell.execute_reply":"2025-10-19T21:27:39.027248Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#2E8B57; color:white; padding:10px 20px; border-radius:8px; font-family: 'Arial', sans-serif; margin-top:20px; margin-bottom:10px; box-shadow: 0 3px 6px rgba(0,0,0,0.1);\">\n    <h2 style=\"margin:0; font-size:24px;\">8) Model Evaluation & Performance Analysis</h2>\n    <p style=\"color:white; font-size:16px; margin-top:5px; line-height:1.4;\">\n        After completing both training phases (before and after fine-tuning), the models were thoroughly evaluated to measure performance and compare improvements.\n        The evaluation includes three main components:\n    </p>\n    <ul style=\"font-size:16px; line-height:1.5; color:white; margin-left:15px;\">\n        <li><strong>Accuracy Comparison</strong> — Measuring accuracy for each model <strong>before and after fine-tuning</strong> to visualize the improvement in prediction performance.</li>\n        <li><strong>Confusion Matrix</strong> — Generated for both model versions to analyze classification behavior, identify misclassified disease categories, and evaluate class-level performance.</li>\n        <li><strong>Training Curves (Loss & Accuracy)</strong> — Showing <strong>Training vs Validation Loss and Accuracy</strong> for each model, helping to assess overfitting, underfitting, and general learning stability.</li>\n    </ul>\n    <p style=\"color:white; font-size:16px; margin-top:5px; line-height:1.4;\">\n        These evaluation metrics provide a clear understanding of how fine-tuning impacted the model’s learning quality and its ability to generalize on unseen plant disease images.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"model_before = load_model(\"mobilenetv3_before_finetuning.keras\")\nloss, accuracy = model_before.evaluate(val_image_generator, verbose=1)\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"Test Loss: {loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T21:41:45.791300Z","iopub.execute_input":"2025-10-19T21:41:45.791558Z","iopub.status.idle":"2025-10-19T21:42:03.450532Z","shell.execute_reply.started":"2025-10-19T21:41:45.791539Z","shell.execute_reply":"2025-10-19T21:42:03.449965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_after = load_model(\"mobilenetv3_after_finetuning.keras\")\nloss, accuracy = model_after.evaluate(val_image_generator, verbose=1)\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"Test Loss: {loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T21:43:21.884980Z","iopub.execute_input":"2025-10-19T21:43:21.885271Z","iopub.status.idle":"2025-10-19T21:43:39.979783Z","shell.execute_reply.started":"2025-10-19T21:43:21.885250Z","shell.execute_reply":"2025-10-19T21:43:39.979131Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#3CB371; color:white; padding:10px 20px; border-radius:8px; font-family: 'Arial', sans-serif; margin-top:15px; margin-bottom:10px; box-shadow: 0 3px 6px rgba(0,0,0,0.1);\"><h3 style=\"margin:0; font-size:20px;\">8.a) Confusion Matrix Visualization</h3><p style=\"color:white; font-size:15px; margin-top:5px; line-height:1.4;\">The image displays a confusion matrix titled <strong>\"Confusion Matrix - Before Fine-Tuning\"</strong>, illustrating the performance of a plant disease classification model. Each cell shows how many instances of a true label were predicted as a specific class.</p><ul style=\"font-size:15px; line-height:1.5; color:white; margin-left:15px;\"><li><strong>Diagonal cells</strong> — Represent <strong>True Positives (TP)</strong>, where predictions match actual disease labels.</li><li><strong>Off-diagonal cells</strong> — Indicate misclassifications, including <strong>False Positives (FP)</strong> and <strong>False Negatives (FN)</strong>.</li><li><strong>Color intensity</strong> — Reflects the number of samples per cell, with darker shades indicating higher counts.</li></ul><p style=\"color:white; font-size:15px; margin-top:5px; line-height:1.4;\">This matrix helps pinpoint which plant disease categories are well predicted and which ones need further fine-tuning, guiding model optimization and dataset balancing.</p></div>","metadata":{}},{"cell_type":"code","source":"val_image_generator.shuffle = False\npred_before = model_before.predict(val_image_generator)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T22:26:54.214555Z","iopub.execute_input":"2025-10-19T22:26:54.214878Z","iopub.status.idle":"2025-10-19T22:27:02.218308Z","shell.execute_reply.started":"2025-10-19T22:26:54.214853Z","shell.execute_reply":"2025-10-19T22:27:02.217757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_after = model_after.predict(val_image_generator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T22:27:04.667084Z","iopub.execute_input":"2025-10-19T22:27:04.667326Z","iopub.status.idle":"2025-10-19T22:27:11.426411Z","shell.execute_reply.started":"2025-10-19T22:27:04.667309Z","shell.execute_reply":"2025-10-19T22:27:11.425709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"true_labels = val_image_generator.classes\n\npred_before_classes = np.argmax(pred_before, axis=1)\npred_after_classes = np.argmax(pred_after, axis=1)\n\ncm_before = confusion_matrix(true_labels, pred_before_classes)\ncm_after = confusion_matrix(true_labels, pred_after_classes)\n\nplt.figure(figsize=(12, 5))\n\n# ---------- FIGURE 1 (Before) ----------\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm_before, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=class_names, yticklabels=class_names)\nplt.title(\"Confusion Matrix - Before Fine-Tuning\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()\n\n# ---------- FIGURE 2 (After) ----------\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm_after, annot=True, fmt=\"d\", cmap=\"Greens\",\n             xticklabels=class_names, yticklabels=class_names)\nplt.title(\"Confusion Matrix - After Fine-Tuning\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T22:27:14.571139Z","iopub.execute_input":"2025-10-19T22:27:14.571848Z","iopub.status.idle":"2025-10-19T22:27:15.712104Z","shell.execute_reply.started":"2025-10-19T22:27:14.571825Z","shell.execute_reply":"2025-10-19T22:27:15.711329Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#3CB371; color:white; padding:10px 20px; border-radius:8px; font-family: 'Arial', sans-serif; margin-top:15px; margin-bottom:10px; box-shadow: 0 3px 6px rgba(0,0,0,0.1);\">\n    <h3 style=\"margin:0; font-size:20px;\">8.b) Classification Report</h3>\n    <p style=\"color:white; font-size:15px; margin-top:5px; line-height:1.4;\">\n        The classification report summarizes the model’s performance on each plant disease class, both before and after fine-tuning.\n        It provides metrics such as Precision, Recall, and F1-Score, which are crucial for understanding class-level performance.\n    </p>\n    <ul style=\"font-size:15px; line-height:1.5; color:white; margin-left:15px;\">\n        <li><strong>Precision</strong> — Proportion of correctly predicted instances among all predicted instances of a class.</li>\n        <li><strong>Recall</strong> — Proportion of correctly predicted instances among all actual instances of a class.</li>\n        <li><strong>F1-Score</strong> — Harmonic mean of Precision and Recall, giving a single metric for class performance.</li>\n        <li><strong>Support</strong> — Number of true instances for each class in the dataset.</li>\n    </ul>\n    <p style=\"color:white; font-size:15px; margin-top:5px; line-height:1.4;\">\n        Comparing classification reports before and after fine-tuning helps identify which disease classes benefited most from fine-tuning and which classes still need improvement.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# ---- Classification Report ----\nreport = classification_report(true_labels, pred_before_classes, target_names=class_names)\nprint(\"\\nClassification Report Before Fine-Tuning :\")\nprint(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T22:30:23.444350Z","iopub.execute_input":"2025-10-19T22:30:23.445066Z","iopub.status.idle":"2025-10-19T22:30:23.458555Z","shell.execute_reply.started":"2025-10-19T22:30:23.445040Z","shell.execute_reply":"2025-10-19T22:30:23.457848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- Classification Report ----\nreport = classification_report(true_labels, pred_after_classes, target_names=class_names)\nprint(\"\\nClassification Report After Fine-Tuning :\")\nprint(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T22:30:26.752148Z","iopub.execute_input":"2025-10-19T22:30:26.752411Z","iopub.status.idle":"2025-10-19T22:30:26.765645Z","shell.execute_reply.started":"2025-10-19T22:30:26.752390Z","shell.execute_reply":"2025-10-19T22:30:26.764967Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#3CB371; color:white; padding:10px 20px; border-radius:8px; font-family: 'Arial', sans-serif; margin-top:15px; margin-bottom:10px; box-shadow: 0 3px 6px rgba(0,0,0,0.1);\">\n    <h3 style=\"margin:0; font-size:20px;\">8.c) Training Curves (Loss & Accuracy)</h3>\n    <p style=\"color:white; font-size:15px; margin-top:5px; line-height:1.4;\">\n        The training curves visualize how the model’s loss and accuracy evolved over each epoch for both training and validation sets. \n        This helps to assess learning behavior and detect issues like overfitting or underfitting.\n    </p>\n    <ul style=\"font-size:15px; line-height:1.5; color:white; margin-left:15px;\">\n        <li><strong>Training Loss</strong> — Shows the decrease of loss on the training set.</li>\n        <li><strong>Validation Loss</strong> — Indicates how well the model generalizes to unseen data.</li>\n        <li><strong>Training Accuracy</strong> — Progression of correctly predicted samples during training.</li>\n        <li><strong>Validation Accuracy</strong> — How accurately the model performs on validation data.</li>\n    </ul>\n    <p style=\"color:white; font-size:15px; margin-top:5px; line-height:1.4;\">\n        By examining these curves, we can understand the impact of fine-tuning on model performance and stability over epochs.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Get the training history\nh = hist.history\n\n# Set plotting style\nplt.style.use('ggplot')\nplt.figure(figsize=(12, 8))\nplt.title(\"Training vs Validation Accuracy and Loss Over Epochs (Before - FineTuning- )\")\n\n# Plot training and validation loss\nplt.plot(h['loss'], c='red', label='Training Loss')\nplt.plot(h['val_loss'], c='red', linestyle='--', label='Validation Loss')\n\n# Plot training and validation accuracy\nplt.plot(h['accuracy'], c='blue', label='Training Accuracy')\nplt.plot(h['val_accuracy'], c='blue', linestyle='--', label='Validation Accuracy')\n\nplt.xlabel(\"Number of Epochs\")\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:48:56.100812Z","iopub.execute_input":"2025-10-19T23:48:56.101054Z","iopub.status.idle":"2025-10-19T23:48:56.300544Z","shell.execute_reply.started":"2025-10-19T23:48:56.101038Z","shell.execute_reply":"2025-10-19T23:48:56.299709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the training history\nh = fine_tune_history.history\n\n# Set plotting style\nplt.style.use('ggplot')\nplt.figure(figsize=(12, 8))\nplt.title(\"Training vs Validation Accuracy and Loss Over Epochs (After - FineTuning- )\")\n\n\n# Plot training and validation loss\nplt.plot(h['loss'], c='red', label='Training Loss')\nplt.plot(h['val_loss'], c='red', linestyle='--', label='Validation Loss')\n\n# Plot training and validation accuracy\nplt.plot(h['accuracy'], c='blue', label='Training Accuracy')\nplt.plot(h['val_accuracy'], c='blue', linestyle='--', label='Validation Accuracy')\n\nplt.xlabel(\"Number of Epochs\")\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:48:53.039193Z","iopub.execute_input":"2025-10-19T23:48:53.039593Z","iopub.status.idle":"2025-10-19T23:48:53.261901Z","shell.execute_reply.started":"2025-10-19T23:48:53.039571Z","shell.execute_reply":"2025-10-19T23:48:53.261196Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#2E8B57; color:white; padding:10px 20px; border-radius:8px; font-family: 'Arial', sans-serif; margin-top:20px; margin-bottom:10px; box-shadow: 0 3px 6px rgba(0,0,0,0.1);\">\n    <h2 style=\"margin:0; font-size:24px;\">9) Testing Images from Google</h2>\n    <p style=\"color:white; font-size:16px; margin-top:5px; line-height:1.4;\">\n        To further evaluate the model's generalization capabilities, additional plant disease images were collected from Google and used as test samples.\n        This step ensures the model is robust and can handle real-world images beyond the training dataset.\n    </p>\n    <ul style=\"font-size:16px; line-height:1.5; color:white; margin-left:15px;\">\n        <li><strong>Image Selection</strong> — Relevant images for each plant disease category were carefully selected to match dataset conditions.</li>\n        <li><strong>Preprocessing</strong> — All images were resized, normalized, and preprocessed similarly to the training data for consistency.</li>\n        <li><strong>Model Evaluation</strong> — Predictions were generated for these images and compared to actual disease labels to assess real-world performance.</li>\n    </ul>\n    <p style=\"color:white; font-size:16px; margin-top:5px; line-height:1.4;\">\n        Testing on external images provides insights into the model’s robustness, highlights potential limitations, and confirms whether fine-tuning improvements translate to unseen, real-world data.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"def PredictImageFromPath(image_path,actual):\n\n     # Load and preprocess the image\n    img = load_img(image_path, target_size=(WIDTH,HEIGHT))\n    img_array = preprocess_input(img_to_array(img))\n    img_array_exp = np.expand_dims(img_array, axis=0) # Add batch dimension\n\n    # Make prediction\n    pred = model_after.predict(img_array_exp, verbose=0)\n    class_index = np.argmax(pred, axis=1)[0]\n    label = class_names[class_index]    \n\n    # Display the image with predicted label\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.title(f\"Predicted: {label} \\n Actual: {actual}\")\n    plt.axis('off')\n    plt.show()\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:18:52.704333Z","iopub.execute_input":"2025-10-19T23:18:52.704604Z","iopub.status.idle":"2025-10-19T23:18:52.709639Z","shell.execute_reply.started":"2025-10-19T23:18:52.704581Z","shell.execute_reply":"2025-10-19T23:18:52.708912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PredictImageFromPath(\"/kaggle/input/last-test-images/Test_Images/bacterial pepper/Pepper-Bacterial-Leaf-Spot-Leaf.jpg\",\"Pepper Bell Bacterial Spot\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:36:08.235429Z","iopub.execute_input":"2025-10-19T23:36:08.236148Z","iopub.status.idle":"2025-10-19T23:36:08.474108Z","shell.execute_reply.started":"2025-10-19T23:36:08.236122Z","shell.execute_reply":"2025-10-19T23:36:08.473340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PredictImageFromPath(\"/kaggle/input/last-test-images/Test_Images/bacterial pepper/figure-2-tattered appearance on the affected leaves-893x595.jpg\",\"Pepper Bell Bacterial Spot\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:36:20.712240Z","iopub.execute_input":"2025-10-19T23:36:20.712496Z","iopub.status.idle":"2025-10-19T23:36:20.932403Z","shell.execute_reply.started":"2025-10-19T23:36:20.712477Z","shell.execute_reply":"2025-10-19T23:36:20.931642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PredictImageFromPath(\"/kaggle/input/last-test-images/Test_Images/Tomato Late blight/IMG_5813.jpg\",\"Tomato Late Blight\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:37:52.644987Z","iopub.execute_input":"2025-10-19T23:37:52.645262Z","iopub.status.idle":"2025-10-19T23:37:52.871302Z","shell.execute_reply.started":"2025-10-19T23:37:52.645240Z","shell.execute_reply":"2025-10-19T23:37:52.870624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PredictImageFromPath(\"/kaggle/input/last-test-images/Test_Images/Tomato Late blight/late_blight_tomato_leaf5x12001-1.jpg\",\"Tomato Late Blight\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:37:58.606047Z","iopub.execute_input":"2025-10-19T23:37:58.606606Z","iopub.status.idle":"2025-10-19T23:37:58.875300Z","shell.execute_reply.started":"2025-10-19T23:37:58.606583Z","shell.execute_reply":"2025-10-19T23:37:58.874485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PredictImageFromPath(\"/kaggle/input/last-test-images/Test_Images/tomato yellow virus/original.jpg\",\"Tomato Yellow Leaf Curl Virus\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:38:22.485508Z","iopub.execute_input":"2025-10-19T23:38:22.485790Z","iopub.status.idle":"2025-10-19T23:38:22.689616Z","shell.execute_reply.started":"2025-10-19T23:38:22.485761Z","shell.execute_reply":"2025-10-19T23:38:22.688897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PredictImageFromPath(\"/kaggle/input/last-test-images/Test_Images/tomato yellow virus/thumb.jpg\",\"Tomato Yellow Leaf Virus\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:38:36.507016Z","iopub.execute_input":"2025-10-19T23:38:36.507463Z","iopub.status.idle":"2025-10-19T23:38:36.712045Z","shell.execute_reply.started":"2025-10-19T23:38:36.507441Z","shell.execute_reply":"2025-10-19T23:38:36.711275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PredictImageFromPath(\"/kaggle/input/last-test-images/Test_Images/potato leaf healthy/The-three-Sample-leaves-of-potato-are-a-leaf-affected-by-Light-Blight-b-leaf.jpg\",\"Potato Healthy\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:39:22.395277Z","iopub.execute_input":"2025-10-19T23:39:22.395913Z","iopub.status.idle":"2025-10-19T23:39:22.604688Z","shell.execute_reply.started":"2025-10-19T23:39:22.395887Z","shell.execute_reply":"2025-10-19T23:39:22.603971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PredictImageFromPath(\"/kaggle/input/last-test-images/Test_Images/potato leaf healthy/original.jpg\",\"Potato Healthy\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:39:53.902999Z","iopub.execute_input":"2025-10-19T23:39:53.903270Z","iopub.status.idle":"2025-10-19T23:39:54.108367Z","shell.execute_reply.started":"2025-10-19T23:39:53.903249Z","shell.execute_reply":"2025-10-19T23:39:54.107597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PredictImageFromPath(\"/kaggle/input/last-test-images/Test_Images/septoria leaf spot/A-tomato-leaf-with-Septoria-leaf-spot-which-is-a-fungal-disease.png\",\"Tomato Septoria  Leaf\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:40:42.124542Z","iopub.execute_input":"2025-10-19T23:40:42.125273Z","iopub.status.idle":"2025-10-19T23:40:42.336454Z","shell.execute_reply.started":"2025-10-19T23:40:42.125245Z","shell.execute_reply":"2025-10-19T23:40:42.335754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PredictImageFromPath(\"/kaggle/input/last-test-images/Test_Images/septoria leaf spot/tomato-septoria-leaf-spot-cropped.jpg\",\"Tomato Septoria  Leaf\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:41:04.459786Z","iopub.execute_input":"2025-10-19T23:41:04.460283Z","iopub.status.idle":"2025-10-19T23:41:04.682981Z","shell.execute_reply.started":"2025-10-19T23:41:04.460261Z","shell.execute_reply":"2025-10-19T23:41:04.682205Z"}},"outputs":[],"execution_count":null}]}